<h1 align="center">
Outils Formels Avancés 2024</h1>
<div>
<td> 
<img src="./read_me_img/logo.png"></td>
<h2 style="white-space: nowrap">Project: AI Fact Checker</h2></td>
<hr style="clear:both">
<p style="font-size:0.85em; margin:2px; text-align:justify">
<br>
<br>
</div>

This repository contains all the informations related to our Fact AI checking project for the course of Outils Formels Avancés 2024 - Master Program at the University of Geneva. 

## Introduction: 

The goal of this project was to create a fact checker to evaluate LLMS, therefore we have created known performance metrics such as rouge metric, sentiment analysis, fact checking... allowing to prompt engineer text generative models to generate well structured answer that we can then check using our metrics.

## Table of contents 


* [1. ROUGE](#1-performance-metric-1-rouge)
* [2. BLEU]()
* [3. Sentiment Analysis](#2-performance-metrix-2-sentiment-analysis)
* [4. Fact Checking](#3-performance-metric-3-fact-checking)
* [5. BERT Score]()
* [6. Natural Language Inference (NLI)]()

### Performance metrics:

**Why?** We want to evaluate text generating LLMs, to do so, we need to define precise performance metrics, below we have defined 6 performance metrics, all of them have corresponding notebooks, in order to use them.

-----------

#### **Guide of use of the performance metrics**

- **1. ROUGE** >Metrics>1_ROUGE.ipynb
- **2. BLUE** >Metrics>2_BLUE.ipynb 
- **3. Fact Checking:** >Notebooks>Metric_3_fact_checking.ipynb 