{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Notebook 2: Compute the results:**\n",
    "\n",
    "This notebook contains the necessary code to compute all the metrics for one LLM at the time, the final results are kept in a folder and merged at the end. \n",
    "\n",
    "!! Do not push any modifications (i.e added results of prompts - keep it clean)\n",
    "\n",
    "**Notebook should execute without a problem on VS code EXCEPT for the bleu library (normalize_ function error) - please try to fix. If problem persists, just run it on colab, works fine.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MODEL_NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip installbert -score\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries here for clarity\n",
    "import pandas as pd\n",
    "import bert_score\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all datasets here for clarity\n",
    "xsum_sample = pd.read_csv(\"./data/dataset_sample_summaries.csv\") # delete . if on colab\n",
    "sentiment = pd.read_csv(\"./data/dataset_sample_movie_reviews.csv\") # Adjust path if necessary\n",
    "fact_checking = pd.read_csv(\"./data/dataset_sample_fact_checking.csv\") # Adjust path if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1: Generate all the prompts:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1: Prompt for ROUGE, BLEU, BERT and ROBERTA metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The full cost of damage in Newton Stewart, one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fire alarm went off at the Holiday Inn in Ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ferrari appeared in a position to challenge un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>John Edward Bates, formerly of Spalding, Linco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Patients and staff were evacuated from Cerahpa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Simone Favaro got the crucial try with the las...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Veronica Vanessa Chango-Alverez, 31, was kille...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Belgian cyclist Demoitie died after a collisio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gundogan, 26, told BBC Sport he \"can see the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The crash happened about 07:20 GMT at the junc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document\n",
       "0  The full cost of damage in Newton Stewart, one...\n",
       "1  A fire alarm went off at the Holiday Inn in Ho...\n",
       "2  Ferrari appeared in a position to challenge un...\n",
       "3  John Edward Bates, formerly of Spalding, Linco...\n",
       "4  Patients and staff were evacuated from Cerahpa...\n",
       "5  Simone Favaro got the crucial try with the las...\n",
       "6  Veronica Vanessa Chango-Alverez, 31, was kille...\n",
       "7  Belgian cyclist Demoitie died after a collisio...\n",
       "8  Gundogan, 26, told BBC Sport he \"can see the f...\n",
       "9  The crash happened about 07:20 GMT at the junc..."
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ROUGE: Generate the results by copy pasting the following prompt:\n",
    "xsum_sample[['document']]\n",
    "# Click on the icon next to *document* (convert this dataframe to an interactive table) - then select (right) copy table and select JSON and copy - paste the result in the cell below  replacing **PASTE_DOCUMENTS_HERE**\n",
    "# Then copy the entire cell and prompt the LLM\n",
    "\n",
    "# PROMPT:\n",
    "# Please generate a summary in one line (max 25 words) for each of the following documents: PASTE_DOCUMENTS_HERE, please just return the answer as the following: results={\"generated_summary\":[\"\",\"\",\"\",\"\",\"\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paste RESULTS here\n",
    "# Example usage:\n",
    "results_rouge_bleu_bert_roberta={\"generated_summary\":[\"\",\"\",\"\",\"\",\"\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2: Prompt for sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I wanted to like this movie. But it falls apar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Even if it were remotely funny, this mouldy wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is an excellent film and one should not b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Despite having known people who are either gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One word: suPURRRRb! I don't think I have see ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The early career of Abe Lincoln is beautifully...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I bought this at tower records after seeing th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Steven Spielberg produced, wrote, came up with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When I took my seat in the cinema I was in a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I wonder how the actors acted in this movie. A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review\n",
       "0  I wanted to like this movie. But it falls apar...\n",
       "1  Even if it were remotely funny, this mouldy wa...\n",
       "2  This is an excellent film and one should not b...\n",
       "3  Despite having known people who are either gre...\n",
       "4  One word: suPURRRRb! I don't think I have see ...\n",
       "5  The early career of Abe Lincoln is beautifully...\n",
       "6  I bought this at tower records after seeing th...\n",
       "7  Steven Spielberg produced, wrote, came up with...\n",
       "8  When I took my seat in the cinema I was in a c...\n",
       "9  I wonder how the actors acted in this movie. A..."
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment[[\"Review\"]]\n",
    "# Click on the icon next to *Review* (convert this dataframe to an interactive table) - then select (right) copy table and select JSON and copy - paste the result in the cell below  replacing **PASTE_DOCUMENTS_HERE**\n",
    "# Then copy the entire cell and prompt the LLM\n",
    "\n",
    "# PROMPT:\n",
    "#Please classify the following 10 sentences: positive, negative or neutral. Here are the sentences please provide an array for answer i.e: predicted_labels = ['positive','negative',...]:: PASTE_SENTENCES_HERE. please return the answers as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paste RESULTS here\n",
    "# Example usage:\n",
    "result_sentiment_analysis = ['negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'neutral', 'positive', 'negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3: Prompt for fact checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How old is Barack Obama, please give just the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the height of the Eiffel Tower, please...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the population of China?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the capital of Australia?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the capital of Italy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Where was Albert Einstein born?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many films have been directed by Steven Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the population of India?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the capital of Japan?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Prompt\n",
       "0  How old is Barack Obama, please give just the ...\n",
       "1  What is the height of the Eiffel Tower, please...\n",
       "2                     What is the capital of France?\n",
       "3                   What is the population of China?\n",
       "4                  What is the capital of Australia?\n",
       "5                      What is the capital of Italy?\n",
       "6                    Where was Albert Einstein born?\n",
       "7  How many films have been directed by Steven Sp...\n",
       "8                   What is the population of India?\n",
       "9                      What is the capital of Japan?"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_checking[['Prompt']]\n",
    "# Click on the icon next to *document* (convert this dataframe to an interactive table) - then select (right) copy table and select JSON and copy - paste the result in the cell below  replacing **PASTE_DOCUMENTS_HERE**\n",
    "# Then copy the entire cell and prompt the LLM\n",
    "\n",
    "# PROMPT:\n",
    "# Please answer the following questions:COPY_QUESTIONS please answer in the following format: arr= [answer_1,answer_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paste RESULTS here\n",
    "# Example usage:\n",
    "result_fact_checking = [61, 324, \"Paris\", 1403500365, \"Canberra\", \"Rome\", \"Ulm, Kingdom of Württemberg, German Empire\", 35, 1380004385, \"Tokyo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2: Generate all the results:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1: Result for ROUGE, BLEU, BERT and ROBERTA metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_export_rouge(model_name, results):\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "    # Create DataFrame from results dictionary\n",
    "    data = pd.DataFrame(results)\n",
    "    # Calculate ROUGE scores\n",
    "    data[\"r1_fscore\"] = data.apply(lambda row: scorer.score(row[\"summary\"], row[\"generated_summary\"])['rouge1'][2], axis=1)\n",
    "    # Calculate mean ROUGE-1 score\n",
    "    mean_rouge_1 = round(data[\"r1_fscore\"].mean(), 2)\n",
    "    # Create DataFrame with mean ROUGE-1 score and model name\n",
    "    df = pd.DataFrame({\n",
    "        \"model_name\": [model_name],\n",
    "        \"ROUGE\": [mean_rouge_1]\n",
    "    })\n",
    "    # Export to CSV\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_result_rouge = pd.DataFrame.from_dict(results_rouge_bleu_bert_roberta).rename({\"summary_text\": \"generated_summary\"}, axis=1).join(pd.DataFrame.from_dict(xsum_sample))[[\"generated_summary\", \"summary\", \"document\"]]\n",
    "df_rouge_result = calculate_and_export_rouge(model_name, opt_result_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_export_bleu(model_name, results):\n",
    "    # Extract reference summaries and generated summaries\n",
    "    references = results[\"summary\"].tolist()\n",
    "    hypotheses = results[\"generated_summary\"].tolist()\n",
    "\n",
    "    # Calculate BLEU score with smoothing\n",
    "    smoothie = SmoothingFunction().method7\n",
    "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smoothie,)\n",
    "\n",
    "    # Create DataFrame with BLEU score and model name\n",
    "    df = pd.DataFrame({\n",
    "        \"model_name\": [model_name],\n",
    "        \"BLEU\": [bleu_score]\n",
    "    })\n",
    "\n",
    "    # Export to CSV\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_result_rouge = pd.DataFrame.from_dict(results_rouge_bleu_bert_roberta).rename({\"summary_text\": \"generated_summary\"}, axis=1).join(pd.DataFrame.from_dict(xsum_sample))[[\"generated_summary\", \"summary\", \"document\"]]\n",
    "df_blue_result = calculate_and_export_rouge(model_name, opt_result_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate BERTScore precision\n",
    "def calculate_bert_precision(model_name,data):\n",
    "    # Extract reference summaries and generated summaries\n",
    "    references = data[\"summary\"].tolist()\n",
    "    hypotheses = data[\"generated_summary\"].tolist()\n",
    "\n",
    "    # Compute BERTScore precision\n",
    "    precision, _, _ = bert_score.score(hypotheses, references, lang=\"en\")\n",
    "\n",
    "    BertScore = precision.mean().item()\n",
    "    # Create DataFrame with BLEU score and model name\n",
    "    df = pd.DataFrame({\n",
    "        \"model_name\": [model_name],\n",
    "        \"BERTSCORE\": [BertScore]\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "opt_result = pd.DataFrame.from_dict(results_rouge_bleu_bert_roberta).rename({\"summary_text\": \"generated_summary\"}, axis=1).join(pd.DataFrame.from_dict(xsum_sample))[[\"generated_summary\", \"summary\", \"document\"]]\n",
    "df_bert_score_results = calculate_bert_precision(model_name, opt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_semantic_similarity(data, model_name):\n",
    "    # Load a pre-trained Sentence Transformer model\n",
    "    model = SentenceTransformer('roberta-base-nli-stsb-mean-tokens')\n",
    "\n",
    "    # Define function to calculate semantic similarity score\n",
    "    def compute_similarity(generated_sentence, reference_sentence):\n",
    "        # Encode sentences into embeddings\n",
    "        generated_embedding = model.encode(generated_sentence, convert_to_tensor=True)\n",
    "        reference_embedding = model.encode(reference_sentence, convert_to_tensor=True)\n",
    "\n",
    "        # Compute cosine similarity between embeddings\n",
    "        cosine_similarity = util.pytorch_cos_sim(generated_embedding, reference_embedding)\n",
    "        return cosine_similarity.item()\n",
    "\n",
    "    # Ensure both lists have the same length\n",
    "    assert len(data[\"generated_summary\"]) == len(data[\"summary\"]), \"Lengths of generated_summary and summary lists must be the same\"\n",
    "\n",
    "    # Calculate semantic similarity for each pair of generated and reference sentences in the data\n",
    "    similarity_scores = []\n",
    "    for generated_sentence, reference_sentence in zip(data[\"generated_summary\"], data[\"summary\"]):\n",
    "        similarity_score = compute_similarity(generated_sentence, reference_sentence)\n",
    "        similarity_scores.append(similarity_score)\n",
    "\n",
    "    # Compute the mean of the similarity scores\n",
    "    mean_similarity = sum(similarity_scores) / len(similarity_scores)\n",
    "\n",
    "    # Create DataFrame with model name and mean similarity score\n",
    "    df = pd.DataFrame({\n",
    "        \"model_name\": [model_name],\n",
    "        \"ROBERTA\": [mean_similarity]\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_robert_results = calculate_semantic_similarity(opt_result,model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2: Result for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_export_sent_analysis(model_name, opt_result):\n",
    "    # Convert both predicted and ground truth labels to lowercase\n",
    "    predicted_labels = [label.lower() for label in opt_result]\n",
    "    sentiment['Predicted_Labels'] = predicted_labels\n",
    "    sentiment['Ground_Truth_Label'] = sentiment['Ground_Truth_Label'].str.lower()\n",
    "    # Count correct predictions\n",
    "    correct_predictions = sum(sentiment['Ground_Truth_Label'] == sentiment['Predicted_Labels'])\n",
    "    total_reviews = len(sentiment)\n",
    "    accuracy = correct_predictions / total_reviews\n",
    "    # Create DataFrame with accuracy and model name\n",
    "    new_data = {\n",
    "        'model_name': model_name,\n",
    "        'SENTIMENT': [accuracy]\n",
    "    }\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment_result = calculate_and_export_sent_analysis(model_name, result_sentiment_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3: Result for Fact checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_export_fact_checking(model_name, results):\n",
    "    def similarity_metric(array1, array2):\n",
    "        count = 0\n",
    "        for elem1 in array1:\n",
    "            if elem1 in array2:\n",
    "                count += 1\n",
    "        similarity_score = count / len(array1)\n",
    "        return similarity_score\n",
    "\n",
    "    # Example ground_truth array\n",
    "    ground_truth = fact_checking[\"Answer\"].to_numpy()\n",
    "    # Convert results to numpy array\n",
    "    array1 = np.array(ground_truth)\n",
    "    array2 = np.array(results, dtype=str)\n",
    "\n",
    "    # Compute similarity score\n",
    "    similarity_score = round(similarity_metric(array1, array2), 2)\n",
    "\n",
    "    # Create DataFrame\n",
    "    new_data = {\n",
    "        'model_name': [model_name],\n",
    "        'FACTCHECK': [similarity_score]\n",
    "    }\n",
    "    df = pd.DataFrame(new_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_checking = calculate_and_export_fact_checking(model_name,  result_fact_checking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3: Concat all the results of all the metrics to one CSV:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model_name  ROUGE\n",
      "0  MODEL_NAME    0.0\n",
      "   model_name  BLEU\n",
      "0  MODEL_NAME  0.14\n",
      "   model_name  SENTIMENT\n",
      "0  MODEL_NAME        0.9\n",
      "   model_name  FACTCHECK\n",
      "0  MODEL_NAME        0.4\n",
      "   model_name  BERTSCORE\n",
      "0  MODEL_NAME   0.852724\n",
      "   model_name   ROBERTA\n",
      "0  MODEL_NAME  0.583766\n"
     ]
    }
   ],
   "source": [
    "print(df_rouge_result)\n",
    "print(df_blue_result) \n",
    "print(df_sentiment_result) \n",
    "print(df_fact_checking) \n",
    "print(df_bert_score_results) \n",
    "print(df_robert_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_rouge_result, df_blue_result, df_sentiment_result, df_fact_checking, df_bert_score_results, df_robert_results are your DataFrames\n",
    "\n",
    "# Merge DataFrames on 'model_name'\n",
    "merged_df = df_rouge_result.merge(df_blue_result, on='model_name')\n",
    "merged_df = merged_df.merge(df_sentiment_result, on='model_name')\n",
    "merged_df = merged_df.merge(df_fact_checking, on='model_name')\n",
    "merged_df = merged_df.merge(df_bert_score_results, on='model_name')\n",
    "merged_df = merged_df.merge(df_robert_results, on='model_name')\n",
    "\n",
    "# Extract the model name\n",
    "model_name = merged_df['model_name'].iloc[0]\n",
    "\n",
    "# Concatenate values into a single row\n",
    "single_row = [model_name] + merged_df.values.flatten().tolist()[1:]\n",
    "\n",
    "# Assuming the columns are in the order of 'model_name', 'ROUGE', 'BLEU', 'SENTIMENT', 'FACTCHECK', 'BERTSCORE', 'ROBERTA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MODEL_NAME', 0.0, 0.14, 0.9, 0.4, 0.8527240753173828, 0.5837660133838654]\n"
     ]
    }
   ],
   "source": [
    "print(single_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert single_row to a DataFrame\n",
    "columns_list = [model_name, 'Rouge', 'Bleu', 'Sentiment', 'Fact checking', 'Bert Score', 'RoberTa']\n",
    "single_row_df = pd.DataFrame([single_row], columns=columns_list)\n",
    "\n",
    "# Export to CSV\n",
    "single_row_df.to_csv(f'./all_results/results_3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
